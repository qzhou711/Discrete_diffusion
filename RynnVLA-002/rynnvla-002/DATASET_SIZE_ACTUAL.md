# 实际数据集规模分析

## 数据来源

直接从 `processed_data` 目录下的文件系统分析得到的数据规模。

---

## 1. 实际数据文件分析

### 1.1 Concate Tokens 文件（当前训练使用的文件）

**文件路径**: `processed_data/concate_tokens/libero_goal_his_2_third_view_wrist_w_state_5_256_abiw.json`

| 属性 | 值 |
|------|-----|
| **文件大小** | 4.08 MB |
| **样本数量** | **25,920** |
| **文件类型** | JSON列表 |
| **数据格式** | 预处理的token数据（每个样本包含 `file`, `len`, `id` 等字段） |

**说明**: 
- 这是当前训练配置中使用的数据文件
- 文件名中的 `abiw` 后缀可能表示 "all"（所有数据合并）
- 训练集、验证集（ind）、验证集（ood）的配置都指向这个文件

---

### 1.2 Convs 文件（原始对话数据，包含数据分割）

在 `processed_data/convs/` 目录下，有原始的分割数据文件：

| 文件 | 路径 |
|------|------|
| **训练集** | `convs/libero_goal_his_2_train_third_view_wrist_w_state_5_256.json` |
| **验证集 (ind)** | `convs/libero_goal_his_2_val_ind_third_view_wrist_w_state_5_256.json` |
| **验证集 (ood)** | `convs/libero_goal_his_2_val_ood_third_view_wrist_w_state_5_256.json` |

**说明**:
- 这些是原始的对话数据文件，包含实际的数据分割
- 需要读取这些文件来确认实际的数据分割情况

---

## 2. 数据分割情况

### 2.1 原始数据分割（从convs文件，实际读取）

**实际数据文件分析结果**:

| 数据集 | 文件大小 | 样本数 | 比例 |
|--------|---------|--------|------|
| **训练集 (train)** | 69.95 MB | **42,909** | 83.87% |
| **验证集 (val_ind)** | 6.47 MB | **4,000** | 7.82% |
| **验证集 (val_ood)** | 6.13 MB | **4,254** | 8.31% |
| **总计** | 82.55 MB | **51,163** | 100% |

**关键发现**:
- ✅ 原始数据总共有 **51,163 个样本**
- ✅ 训练集占约 84%，验证集（ind）约 8%，验证集（ood）约 8%
- ⚠️ **Concate文件只有25,920个样本，远少于原始数据的51,163个样本**
- 📝 差异：51,163 - 25,920 = **25,243 个样本未包含在concate文件中**

---

## 3. 当前训练配置的实际情况

### 3.1 配置文件分析

**配置文件**: `configs/libero_goal/his_2_third_view_wrist_w_state_5_256_pretokenize.yaml`

```yaml
META:
  - path: '../processed_data/concate_tokens/libero_goal_his_2_third_view_wrist_w_state_5_256_abiw.json'
```

**关键发现**:
- ✅ 训练集、验证集（ind）、验证集（ood）的配置都指向**同一个文件**
- ✅ 该文件包含 **25,920 个样本**
- ⚠️ **所有数据集使用相同的数据**（没有实际的数据分割）

### 3.2 训练时的数据规模

由于所有数据集使用相同的配置文件和数据文件：

| 数据集 | 使用的文件 | 样本数 |
|--------|-----------|--------|
| **训练集 (train)** | `libero_goal_his_2_third_view_wrist_w_state_5_256_abiw.json` | **25,920** |
| **验证集 (val_ind)** | `libero_goal_his_2_third_view_wrist_w_state_5_256_abiw.json` | **25,920** |
| **验证集 (val_ood)** | `libero_goal_his_2_third_view_wrist_w_state_5_256_abiw.json` | **25,920** |

**结论**: 
- 所有数据集使用相同的25,920个样本
- 这是预处理后的token数据，已经合并了train/val_ind/val_ood的所有数据
- 在实际训练中，数据集加载器会处理这些数据，但所有数据集都看到相同的样本

---

## 4. 数据预处理流程

### 4.1 数据预处理步骤

根据代码分析，数据预处理流程如下：

1. **原始数据生成** (`action_state_model_conv_generation.py`):
   - 从原始HDF5文件生成对话数据
   - 按照任务和轨迹分割为 train/val_ind/val_ood
   - 保存到 `processed_data/convs/` 目录

2. **Token化** (预处理步骤):
   - 将对话数据转换为token序列
   - 保存到 `processed_data/tokens/` 目录（按split分开）

3. **合并Token数据** (`concate_action_world_model_data_libero.py`):
   - 将所有split的token数据合并
   - 保存到 `processed_data/concate_tokens/` 目录
   - 文件名后缀 `abiw` 可能表示 "all"（所有数据）

### 4.2 当前使用的数据文件

**当前训练使用的文件**: `libero_goal_his_2_third_view_wrist_w_state_5_256_abiw.json`

- 这是合并后的token数据文件
- 包含所有split的数据（train + val_ind + val_ood）
- 样本总数：25,920

---

## 5. 数据集规模总结

### 5.1 当前训练配置

| 数据集 | 样本数 | 说明 |
|--------|--------|------|
| **训练集** | **25,920** | 使用合并文件 |
| **验证集 (ind)** | **25,920** | 使用合并文件 |
| **验证集 (ood)** | **25,920** | 使用合并文件 |

**关键点**:
- 所有数据集使用相同的25,920个样本
- 数据文件是预处理后的token数据
- 没有实际的数据分割（所有数据集看到相同的数据）

### 5.2 原始数据分割（从convs文件，实际数据）

| 数据集 | 实际样本数 | 实际比例 |
|--------|-----------|---------|
| **训练集** | **42,909** | 83.87% |
| **验证集 (ind)** | **4,000** | 7.82% |
| **验证集 (ood)** | **4,254** | 8.31% |
| **总计** | **51,163** | 100% |

**重要对比**:
- 原始数据总数: **51,163** 个样本
- Concate文件样本数: **25,920** 个样本
- **差异**: 25,243 个样本（约49.3%的数据未包含在concate文件中）

**可能的原因**:
1. 数据预处理过程中有采样或过滤
2. Concate文件只包含了部分数据
3. 可能有数据质量过滤或长度过滤
4. 或者concate文件是某个特定子集的数据

---

## 6. 训练配置说明

### 6.1 为什么所有数据集使用相同文件？

这可能是因为：

1. **预tokenize模式**: 数据已经预处理为token格式，所有split的数据合并在一起
2. **数据加载方式**: 数据集加载器可能通过其他方式（如索引）来区分train/val
3. **配置简化**: 使用同一个文件简化配置管理

### 6.2 实际的数据使用

在实际训练中：
- 训练集：使用全部或部分25,920个样本
- 验证集（ind）：可能使用相同的样本，但用于评估
- 验证集（ood）：可能使用相同的样本，但用于评估

**注意**: 这种方式意味着训练集和验证集实际上是相同的，这可能导致过拟合评估不准确。

---

## 7. 建议

### 7.1 数据分割验证

建议检查：
1. 读取 `convs/` 目录下的分割文件，确认实际的样本数
2. 检查数据加载器是否有其他机制来区分train/val数据
3. 确认训练时是否有数据过滤或采样机制

### 7.2 数据使用方式

如果确实所有数据集使用相同数据：
- 考虑使用convs目录下的分割文件
- 或者修改配置，为train/val_ind/val_ood使用不同的数据文件
- 确保训练和验证使用不同的数据，以便准确评估模型性能

---

## 总结

### 关键发现

**当前训练使用的数据**:
- ✅ Concate tokens文件包含 **25,920 个样本**
- ✅ 训练/验证/测试配置都指向这个文件
- ⚠️ 所有数据集使用相同的数据（没有实际分割）

**原始数据的实际情况**:
- ✅ 原始数据总共有 **51,163 个样本**
  - 训练集: 42,909 个样本（83.87%）
  - 验证集 (ind): 4,000 个样本（7.82%）
  - 验证集 (ood): 4,254 个样本（8.31%）
- ⚠️ **Concate文件只包含约50.7%的原始数据**（25,920 / 51,163）
- 📝 有25,243个样本（49.3%）未包含在concate文件中

### 数据规模对比

| 数据来源 | 样本数 | 说明 |
|---------|--------|------|
| **原始数据 (convs)** | 51,163 | 完整的数据集，包含train/val分割 |
| **预处理数据 (concate)** | 25,920 | 当前训练使用的数据 |
| **差异** | -25,243 | 未包含在concate文件中的样本 |

### 建议

1. **数据使用确认**: 
   - 确认concate文件为什么只包含部分数据
   - 检查数据预处理过程中是否有采样或过滤

2. **训练数据评估**:
   - 当前只使用了约50.7%的原始数据
   - 考虑是否需要使用更多数据来训练

3. **数据分割**:
   - 如果需要在训练和验证时使用不同的数据，考虑使用convs目录下的分割文件
   - 或者检查是否有其他方式在数据加载时进行分割

